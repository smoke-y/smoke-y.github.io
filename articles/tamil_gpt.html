<html>
    <link rel="stylesheet" href="..//index.css"/>
    <title>[505]</title>
    <body>
        <div class="header">
            <br>
            <h1>[505]</h1>
            <h3>
                <a href="../index.html">home</a>
                |
                <a href="../html/articles.html">articles</a>
                |
                <a href="../html/opus.html">opus</a>
                |
                <a href="../html/art.html">art</a>
            </h3>
        </div>
        <h1>TAMIL_GPT.HTML</h1>
        <h3 class="article">
            Here's how I trained a GPT in 4 hours on a single <a href="https://youtu.be/MShbP3OpASA?si=our-8VdMI8f-7SKz&t=2997">NVIDIA</a> RTX a4000(16GB).

I started by implementing <a href="https://github.com/karpathy/build-nanogpt">nanoGPT</a> by Karpathy Sensei. nanoGPT is a 12-layer GPT2 which was trained on 8 <a href="https://youtu.be/MShbP3OpASA?si=our-8VdMI8f-7SKz&t=2997">NVIDIA</a> a100(80GB). All I had was a single <a href="https://youtu.be/MShbP3OpASA?si=our-8VdMI8f-7SKz&t=2997">NVIDIA</a> a4000 which my uni was ready to lend me for a day :(

I stumbled across <a href="https://github.com/KellerJordan/modded-nanogpt">modded-nanoGPT</a>: A repo trying to speedrun the training of nanoGPT. They "modded" nanoGPT with QK-normalization, rotary embeddings, skip connections, etc...

I applied these changes to my GPT hoping it would reduce my training time:

1) <a href="https://arxiv.org/abs/2010.04245">QK-normalization</a>: If the QK matrix is not normalized, we can encounter an explosion on a certain dimension(s) in the logits. This might silence other signals, making it harder to learn.

2) Skip connections: The first block is connected to the last block, the second block is connected to the second last, and so on.

3) <a href="https://arxiv.org/abs/2104.09864">Rotary embeddings</a>: Usually, positional embeddings are added before QK matrix projection. Rotary embedding adds positional information after QK matrix projection using a rotation matrix, which results in a more uniform/predictable encoding of positional information.

4) Weight initialization: Reduces the chance of exploding or vanishing gradients.

5) <a href="https://arxiv.org/abs/2109.08668v2">ReLu^2</a> and multiples of 2: ReLu^2 performs slightly better than GELU and keeping the size of the embedding layer in multiples of 2 increases performance.

I started training at 12:00 PM and the GPT crunched around 580 million tokens(<a href="https://github.com/AI4Bharat/indicnlp_corpus?tab=readme-ov-file#text-corpora">dataset</a>) in 4 hours.

<img src="../images/tloss.png">

We can do better!
1) <a href="https://kellerjordan.github.io/posts/muon/">Muon</a> optimizer: This optimizer has been used in modded-nanoGPT for optimizing hidden matrices. I didn't have enough time to play around to find ideal hyperparameters so I used my good old friend Adam.
2) <a href="https://arxiv.org/abs/2412.19437v1">Multi-head latent attention</a>: Used in Deepseek v3, reduces memory overhead while giving good performance.

You can find the code <a href="https://github.com/smoke-y/tamilGPT">here</a> and the weights <a href="https://huggingface.co/smoke-y/tamilGPT">here</a>.
        </h3>
    </body>
</html>
